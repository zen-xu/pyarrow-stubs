import collections.abc
import datetime as dt
from decimal import Decimal
import enum
import importlib._bootstrap  # type: ignore
import io
from os import PathLike
from types import ModuleType
from typing import (
    Any,
    Callable,
    ClassVar,
    Generator,
    Generic,
    ItemsView,
    Iterable,
    KeysView,
    NamedTuple,
    TypeVar,
    ValuesView,
    overload,
)

import _io  # type: ignore
import numpy as np
from numpy.typing import (
    ArrayLike,
    DTypeLike,
    NDArray,
)
import pandas as pd
from pyarrow.compute import (
    CastOptions,
    FunctionOptions,
)
from typing_extensions import (
    Literal,
    TypeAlias,
    TypeGuard,
)

_ArrowType: TypeAlias = int | DataType
_builtin_slice = slice
DEFAULT_BUFFER_SIZE: int
NA: NullScalar
Type_BINARY: _ArrowType
Type_BOOL: _ArrowType
Type_DATE32: _ArrowType
Type_DATE64: _ArrowType
Type_DECIMAL128: _ArrowType
Type_DECIMAL256: _ArrowType
Type_DENSE_UNION: _ArrowType
Type_DICTIONARY: _ArrowType
Type_DOUBLE: _ArrowType
Type_DURATION: _ArrowType
Type_FIXED_SIZE_BINARY: _ArrowType
Type_FIXED_SIZE_LIST: _ArrowType
Type_FLOAT: _ArrowType
Type_HALF_FLOAT: _ArrowType
Type_INT16: _ArrowType
Type_INT32: _ArrowType
Type_INT64: _ArrowType
Type_INT8: _ArrowType
Type_INTERVAL_MONTH_DAY_NANO: _ArrowType
Type_LARGE_BINARY: _ArrowType
Type_LARGE_LIST: _ArrowType
Type_LARGE_STRING: _ArrowType
Type_LIST: _ArrowType
Type_MAP: _ArrowType
Type_NA: _ArrowType
Type_SPARSE_UNION: _ArrowType
Type_STRING: _ArrowType
Type_STRUCT: _ArrowType
Type_TIME32: _ArrowType
Type_TIME64: _ArrowType
Type_TIMESTAMP: _ArrowType
Type_UINT16: _ArrowType
Type_UINT32: _ArrowType
Type_UINT64: _ArrowType
Type_UINT8: _ArrowType
UnionMode_DENSE: int
UnionMode_SPARSE: int
V1: importlib._bootstrap.MetadataVersion
V2: importlib._bootstrap.MetadataVersion
V3: importlib._bootstrap.MetadataVersion
V4: importlib._bootstrap.MetadataVersion
V5: importlib._bootstrap.MetadataVersion
_NULL: NullScalar
__pc: ModuleType | None
_break_traceback_cycle_from_frame: Callable
_default_context_initialized: bool
_default_serialization_context: SerializationContext
_is_path_like: Callable
_pandas_api: _PandasAPIShim
_python_extension_types_registry: list
_registry_nanny: _ExtensionRegistryNanny
_stringify_path: Callable
contextmanager: Callable
cpp_build_info: importlib._bootstrap.BuildInfo
cpp_version: str
cpp_version_info: importlib._bootstrap.VersionInfo
have_signal_refcycle: bool
namedtuple: Callable
builtin_pickle: Callable

class PyCapsule: ...

_Self = TypeVar("_Self")

_Array = TypeVar("_Array", bound=Array)
_ChunkedArray = TypeVar("_ChunkedArray", bound=ChunkedArray)

_T = TypeVar("_T")
_T2 = TypeVar("_T2")
_Scalar = TypeVar("_Scalar", bound=Scalar)

class Array(_PandasConvertible, Generic[_T, _Scalar]):
    _name: Any
    nbytes: int
    null_count: int
    offset: int
    type: DataType[_T]
    def __init__(self) -> None: ...
    def _debug_print(self) -> Any: ...
    @staticmethod
    def _export_to_c(out_ptr: int, out_schema_ptr: int | None = ...) -> Array: ...
    @staticmethod
    def _import_from_c(in_ptr: int, type: DataType | int) -> Array: ...
    def _to_pandas(
        self,
        options: dict[str, Any],
        types_mapper: Callable[[DataType], pd.api.extensions.ExtensionDtype | None]
        | None = ...,
        **kwargs,
    ) -> pd.Series: ...
    def buffers(self) -> list[Buffer | None]: ...
    @overload
    def cast(
        self,
        target_type: Literal["bool", "boolean"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> BooleanArray: ...
    @overload
    def cast(
        self,
        target_type: Literal["i1", "int8"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> Int8Array: ...
    @overload
    def cast(
        self,
        target_type: Literal["i2", "int16"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> Int16Array: ...
    @overload
    def cast(
        self,
        target_type: Literal["i4", "int32"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> Int32Array: ...
    @overload
    def cast(
        self,
        target_type: Literal["i8", "int64"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> Int64Array: ...
    @overload
    def cast(
        self,
        target_type: Literal["u1", "uint8"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> UInt8Array: ...
    @overload
    def cast(
        self,
        target_type: Literal["u2", "uint16"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> UInt16Array: ...
    @overload
    def cast(
        self,
        target_type: Literal["u4", "uint32"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> UInt32Array: ...
    @overload
    def cast(
        self,
        target_type: Literal["u8", "uint64"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> UInt64Array: ...
    @overload
    def cast(
        self,
        target_type: Literal["f2", "halffloat", "float16"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> HalfFloatArray: ...
    @overload
    def cast(
        self,
        target_type: Literal["f4", "float", "float32"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> FloatArray: ...
    @overload
    def cast(
        self,
        target_type: Literal["f8", "double", "float64"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> DoubleArray: ...
    @overload
    def cast(
        self,
        target_type: Literal["string", "str", "utf8"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> StringArray: ...
    @overload
    def cast(
        self,
        target_type: Literal["binary"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> BinaryArray: ...
    @overload
    def cast(
        self,
        target_type: Literal["large_string", "large_str", "large_utf8"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> LargeStringArray: ...
    @overload
    def cast(
        self,
        target_type: Literal["large_binary"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> LargeBinaryArray: ...
    @overload
    def cast(
        self,
        target_type: Literal["date32", "date32[day]"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> Date32Array: ...
    @overload
    def cast(
        self,
        target_type: Literal["date64", "date64[ms]"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> Date64Array: ...
    @overload
    def cast(
        self,
        target_type: Literal["time32[s]", "time32[ms]"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> Time32Array: ...
    @overload
    def cast(
        self,
        target_type: Literal["time64[us]", "time64[ns]"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> Time64Array: ...
    @overload
    def cast(
        self,
        target_type: Literal[
            "timestamp[s]", "timestamp[ms]", "timestamp[us]", "timestamp[ns]"
        ],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> TimestampArray: ...
    @overload
    def cast(
        self,
        target_type: Literal[
            "duration[s]", "duration[ms]", "duration[us]", "duration[ns]"
        ],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> DurationArray: ...
    @overload
    def cast(
        self,
        target_type: Literal["month_day_nano_interval"],
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> MonthDayNanoIntervalArray: ...
    @overload
    def cast(
        self,
        target_type: DataType[_T2] | None = ...,
        safe: bool = ...,
        options: CastOptions = ...,
    ) -> Array[_T2, Scalar[_T2]]: ...
    def dictionary_encode(self, null_encoding: str = ...) -> DictionaryArray: ...
    def diff(self, other: Array) -> str: ...
    def drop_null(self: _Array) -> _Array: ...
    def equals(self, other: Array) -> bool: ...
    def fill_null(self: _Array, fill_value: _T) -> _Array: ...
    def filter(
        self: _Array,
        mask: list[bool] | BooleanArray,
        *,
        null_selection_behavior: Literal["drop", "emit_null"] = ...,
    ) -> _Array: ...
    def format(self, **kwargs) -> Any: ...
    @staticmethod
    def from_buffers(
        type: DataType,
        length: int,
        buffers: list[Buffer],
        null_count: int = ...,
        offset: int = ...,
        children: list[_Array] = ...,
    ) -> _Array: ...
    @staticmethod
    def from_pandas(
        obj: pd.Series | ArrayLike,
        mask: BooleanArray = ...,
        type: DataType[_T2] = ...,
        safe: bool = ...,
        memory_pool: MemoryPool = ...,
    ) -> Array[_T2, Scalar[_T2]] | ChunkedArray[_T2, Scalar[_T2]]: ...
    def get_total_buffer_size(self) -> int: ...
    def index(
        self,
        value: Scalar | object,
        start: int | None = ...,
        end: int | None = ...,
        *,
        memory_pool: MemoryPool | None = ...,
    ) -> Int64Scalar: ...
    def is_null(self, *, nan_is_null: bool = ...) -> BooleanArray: ...
    def is_valid(self) -> BooleanArray: ...
    def slice(self: _Array, offset: int = ..., length: int | None = ...) -> _Array: ...
    def sum(self, **kwargs) -> Any: ...
    def take(
        self: _Array,
        indices: list[int]
        | IntegerArray
        | NDArray[np.signedinteger | np.unsignedinteger],
    ) -> _Array: ...
    def to_numpy(self, zero_copy_only: bool = ..., writable: bool = ...) -> NDArray: ...
    def to_pylist(self) -> list[_T]: ...
    def to_string(
        self,
        *,
        indent: int = ...,
        top_level_indent: int = ...,
        window: int = ...,
        container_window: int = ...,
        skip_new_lines: bool = ...,
    ) -> str: ...
    def tolist(self) -> list[_T]: ...
    def unique(self: _Array) -> _Array: ...
    def validate(self, *, full: bool = ...) -> None: ...
    def value_counts(self) -> StructArray: ...
    @overload
    def view(self, target_type: Literal["bool", "boolean"]) -> BooleanArray: ...
    @overload
    def view(self, target_type: Literal["i1", "int8"]) -> Int8Array: ...
    @overload
    def view(self, target_type: Literal["i2", "int16"]) -> Int16Array: ...
    @overload
    def view(self, target_type: Literal["i4", "int32"]) -> Int32Array: ...
    @overload
    def view(self, target_type: Literal["i8", "int64"]) -> Int64Array: ...
    @overload
    def view(self, target_type: Literal["u1", "uint8"]) -> UInt8Array: ...
    @overload
    def view(self, target_type: Literal["u2", "uint16"]) -> UInt16Array: ...
    @overload
    def view(self, target_type: Literal["u4", "uint32"]) -> UInt32Array: ...
    @overload
    def view(self, target_type: Literal["u8", "uint64"]) -> UInt64Array: ...
    @overload
    def view(
        self, target_type: Literal["f2", "halffloat", "float16"]
    ) -> HalfFloatArray: ...
    @overload
    def view(self, target_type: Literal["f4", "float", "float32"]) -> FloatArray: ...
    @overload
    def view(self, target_type: Literal["f8", "double", "float64"]) -> DoubleArray: ...
    @overload
    def view(self, target_type: Literal["string", "str", "utf8"]) -> StringArray: ...
    @overload
    def view(self, target_type: Literal["binary"]) -> BinaryArray: ...
    @overload
    def view(
        self, target_type: Literal["large_string", "large_str", "large_utf8"]
    ) -> LargeStringArray: ...
    @overload
    def view(self, target_type: Literal["large_binary"]) -> LargeBinaryArray: ...
    @overload
    def view(self, target_type: Literal["date32", "date32[day]"]) -> Date32Array: ...
    @overload
    def view(self, target_type: Literal["date64", "date64[ms]"]) -> Date64Array: ...
    @overload
    def view(self, target_type: Literal["time32[s]", "time32[ms]"]) -> Time32Array: ...
    @overload
    def view(self, target_type: Literal["time64[us]", "time64[ns]"]) -> Time64Array: ...
    @overload
    def view(
        self,
        target_type: Literal[
            "timestamp[s]", "timestamp[ms]", "timestamp[us]", "timestamp[ns]"
        ],
    ) -> TimestampArray: ...
    @overload
    def view(
        self,
        target_type: Literal[
            "duration[s]", "duration[ms]", "duration[us]", "duration[ns]"
        ],
    ) -> DurationArray: ...
    @overload
    def view(
        self,
        target_type: Literal["month_day_nano_interval"],
    ) -> MonthDayNanoIntervalArray: ...
    @overload
    def view(self, target_type: DataType[_T2]) -> Array[_T2, Scalar[_T2]]: ...
    def __array__(self, dtype: DTypeLike = ...) -> NDArray: ...
    def __eq__(self, other) -> bool: ...
    @overload
    def __getitem__(self, key: int) -> _Scalar: ...
    @overload
    def __getitem__(self: _Array, key: _builtin_slice) -> _Array: ...
    def __iter__(self) -> Generator[_Scalar, None, None]: ...
    def __len__(self) -> int: ...
    def __sizeof__(self) -> int: ...

class ArrowCancelled(ArrowException):
    def __init__(self, message: str, signum: int = ...) -> None: ...

class ArrowCapacityError(ArrowException): ...
class ArrowException(Exception): ...
class ArrowIndexError(IndexError, ArrowException): ...
class ArrowInvalid(ValueError, ArrowException): ...
class ArrowKeyError(KeyError, ArrowException): ...
class ArrowMemoryError(MemoryError, ArrowException): ...
class ArrowNotImplementedError(NotImplementedError, ArrowException): ...
class ArrowSerializationError(ArrowException): ...
class ArrowTypeError(TypeError, ArrowException): ...

ArrowIOError = IOError

class BaseExtensionType(DataType[_T]):
    extension_name: str
    storage_type: DataType[_T]
    def __init__(self, *args, **kwargs) -> None: ...
    @overload
    def wrap_array(
        self, storage: Array[_T2, _Scalar]
    ) -> ExtensionArray[_T, ExtensionScalar[_T], Array[_T2, _Scalar]]: ...
    @overload
    def wrap_array(
        self, storage: ChunkedArray[_T2, _Scalar]
    ) -> ChunkedArray[_T, ExtensionScalar[_T]]: ...

class BaseListArray(Array[list[_T], _Scalar]):
    def __init__(self, *args, **kwargs) -> None: ...
    def flatten(self) -> Array[_T, _Scalar]: ...
    def value_lengths(self) -> Int32Array: ...
    def value_parent_indices(self) -> Int64Array: ...

class BinaryArray(Array[_T, BinaryScalar]):
    total_values_length: int

class BinaryScalar(Scalar[_T]):
    def as_buffer(self) -> Buffer: ...
    def as_py(self) -> _T: ...

class BooleanArray(Array[bool, BooleanScalar]):
    false_count: int
    true_count: int

class BooleanScalar(Scalar[bool]): ...

class Buffer(_Weakrefable):
    address: int
    is_cpu: bool
    is_mutable: bool
    parent: Buffer | None
    size: int
    def equals(self, other) -> bool: ...
    def hex(self) -> bytes: ...
    def slice(self, offset: int = ..., length: int | None = ...) -> Buffer: ...
    def to_pybytes(self) -> bytes: ...
    def __eq__(self, other) -> bool: ...
    @overload
    def __getitem__(self, key: int) -> int: ...
    @overload
    def __getitem__(self, key: _builtin_slice) -> Buffer: ...
    def __len__(self) -> int: ...

class BufferOutputStream(NativeFile):
    def getvalue(self) -> Buffer: ...

class BufferReader(NativeFile): ...
class BufferedIOBase(_io._BufferedIOBase, io.IOBase): ...

class BufferedInputStream(NativeFile):
    def __init__(
        self, stream: NativeFile, buffer_size: int, memory_pool: MemoryPool | None = ...
    ) -> None: ...
    def detach(self) -> NativeFile: ...

class BufferedOutputStream(NativeFile):
    def __init__(
        self, stream: NativeFile, buffer_size: int, memory_pool: MemoryPool | None = ...
    ) -> None: ...
    def detach(self) -> NativeFile: ...

class BuildInfo(NamedTuple):
    build_type: str
    compiler_flags: str
    compiler_id: str
    compiler_version: str
    full_so_version: str
    git_description: str
    git_id: str
    package_kind: str
    so_version: str
    version: str
    version_info: str

class ChunkedArray(_PandasConvertible, Generic[_T, _Scalar]):
    _name: str | None
    chunks: list[Array[_T, _Scalar]]
    nbytes: int
    null_count: int
    num_chunks: int
    type: DataType[_T]
    @property
    def data(self: _ChunkedArray) -> _ChunkedArray: ...
    def _to_pandas(self, options, types_mapper=..., **kwargs) -> Any: ...
    def cast(self, target_type=..., safe=..., options=...) -> Any: ...
    def chunk(self, i: int) -> Array[_T, _Scalar]: ...
    def combine_chunks(self, memory_pool: MemoryPool | None = ...) -> Table: ...
    def dictionary_encode(
        self: _ChunkedArray, null_encoding: str = ...
    ) -> _ChunkedArray: ...
    def drop_null(self: _ChunkedArray) -> _ChunkedArray: ...
    def equals(self, other) -> bool: ...
    def fill_null(self: _ChunkedArray, fill_value: _T) -> _ChunkedArray: ...
    def filter(
        self: _ChunkedArray,
        mask: list[bool] | BooleanArray,
        *,
        null_selection_behavior: Literal["drop", "emit_null"] = ...,
    ) -> _ChunkedArray: ...
    def flatten(
        self: _ChunkedArray, memory_pool: MemoryPool | None = ...
    ) -> _ChunkedArray: ...
    def format(self, **kwargs) -> str: ...
    def get_total_buffer_size(self) -> int: ...
    def index(
        self,
        value: Scalar | object,
        start: int | None = ...,
        end: int | None = ...,
        *,
        memory_pool: MemoryPool | None = ...,
    ) -> Int64Scalar: ...
    def is_null(self) -> ChunkedArray[bool, BooleanScalar]: ...
    def is_valid(self) -> ChunkedArray[bool, BooleanScalar]: ...
    def iterchunks(self) -> Generator[Array[_T, _Scalar], None, None]: ...
    def length(self) -> int: ...
    def slice(
        self: _ChunkedArray, offset: int = ..., length: int | None = ...
    ) -> _ChunkedArray: ...
    def take(
        self: _ChunkedArray,
        indices: list[int]
        | IntegerArray
        | NDArray[np.signedinteger | np.unsignedinteger],
    ) -> _ChunkedArray: ...
    def to_numpy(self) -> NDArray: ...
    def to_pylist(self) -> list[_T]: ...
    def to_string(
        self,
        *,
        indent: int = ...,
        window: int = ...,
        container_window: int = ...,
        skip_new_lines: bool = ...,
    ) -> str: ...
    def unify_dictionaries(
        self: _ChunkedArray, memory_pool: MemoryPool = ...
    ) -> _ChunkedArray: ...
    def unique(self) -> ChunkedArray[int, Int64Scalar]: ...
    def validate(self, *, full: bool = ...) -> None: ...
    def value_counts(self) -> StructArray: ...
    def __array__(self, dtype: DTypeLike = ...) -> NDArray: ...
    def __eq__(self, other) -> bool: ...
    @overload
    def __getitem__(self, key: int) -> _Scalar: ...
    @overload
    def __getitem__(self: _ChunkedArray, key: _builtin_slice) -> _ChunkedArray: ...
    def __iter__(self) -> Generator[_Scalar, None, None]: ...
    def __len__(self) -> int: ...
    def __sizeof__(self) -> int: ...

_COMPRESSION: TypeAlias = Literal[
    "gzip", "bz2", "brotli", "lz4" "lz4_frame", "lz4_raw", "zstd", "snappy"
]

class Codec(_Weakrefable):
    compression_level: int | None
    name: str
    def __init__(
        self,
        compression: _COMPRESSION,
        compression_level: int | None = ...,
    ) -> None: ...
    @overload
    def compress(
        self,
        buf: Buffer | bytes | memoryview,
        memory_pool: MemoryPool | None = ...,
    ) -> Buffer: ...
    @overload
    def compress(
        self,
        buf: Buffer | bytes | memoryview,
        asbytes: Literal[True] = ...,
        memory_pool: MemoryPool | None = ...,
    ) -> bytes: ...
    def decompress(
        self, buf, decompressed_size=..., asbytes=..., memory_pool=...
    ) -> Any: ...
    @staticmethod
    def default_compression_level(compression: _COMPRESSION) -> int: ...
    @staticmethod
    def detect(path: str | PathLike) -> Codec: ...
    @staticmethod
    def is_available(compression: _COMPRESSION) -> bool: ...
    @staticmethod
    def maximum_compression_level(compression: _COMPRESSION) -> int: ...
    @staticmethod
    def minimum_compression_level(compression: _COMPRESSION) -> int: ...
    @staticmethod
    def supports_compression_level(compression: _COMPRESSION) -> bool: ...

class CompressedInputStream(NativeFile):
    def __init__(
        self,
        stream: str | PathLike | NativeFile | IOBase,
        compression: Literal["bz2", "brotli", "gzip", "lz4", "zstd"],
    ) -> None: ...

class CompressedOutputStream(NativeFile):
    def __init__(
        self,
        stream: str | PathLike | NativeFile | IOBase,
        compression: Literal["bz2", "brotli", "gzip", "lz4", "zstd"],
    ) -> None: ...

class DataType(_Weakrefable, Generic[_T]):
    bit_width: int
    id: int
    num_buffers: int
    num_fields: int
    def _export_to_c(self, out_ptr: int) -> None: ...
    def _import_from_c(self, in_ptr: int) -> Any: ...
    def equals(self, other) -> bool: ...
    def field(self, i: int) -> Field: ...
    def to_pandas_dtype(self) -> DTypeLike: ...
    def __eq__(self, other) -> bool: ...

class Date32Array(NumericArray[dt.date, Date32Scalar]): ...
class Date32Scalar(Scalar[dt.date]): ...
class Date64Array(NumericArray[dt.date, Date64Scalar]): ...
class Date64Scalar(Scalar[dt.date]): ...
class Decimal128Array(FixedSizeBinaryArray): ...
class Decimal128Scalar(Scalar[Decimal]): ...

class Decimal128Type(FixedSizeBinaryType):
    precision: int
    scale: int

class Decimal256Array(FixedSizeBinaryArray): ...
class Decimal256Scalar(Scalar[Decimal]): ...

class Decimal256Type(FixedSizeBinaryType):
    precision: int
    scale: int

class DenseUnionType(UnionType): ...

class DeserializationCallbackError(ArrowSerializationError):
    def __init__(self, message: str, type_id) -> None: ...

class DictionaryArray(Array[dict, DictionaryScalar]):
    dictionary: Any
    indices: Any
    @classmethod
    def __init__(self, *args, **kwargs) -> None: ...
    def dictionary_decode(self: _Array) -> _Array: ...
    def dictionary_encode(self) -> DictionaryArray: ...  # type: ignore
    @staticmethod
    def from_arrays(
        indices: Array | NDArray | pd.Series,
        dictionary: Array | NDArray | pd.Series,
        mask: NDArray | pd.Series = ...,
        ordered: bool = ...,
        from_pandas: bool = ...,
        safe: bool = ...,
        memory_pool: MemoryPool = ...,
    ) -> DictionaryArray: ...
    @staticmethod
    def from_buffers(  # type: ignore
        type: DataType,
        length: int,
        buffers: list[Buffer],
        dictionary: Array | NDArray | pd.Series,
        null_count: int = ...,
        offset: int = ...,
    ) -> DictionaryArray: ...

class DictionaryMemo(_Weakrefable): ...

class DictionaryScalar(Scalar[dict]):
    dictionary: Any
    index: Any
    value: Any

class DictionaryType(DataType):
    index_type: Any
    ordered: Any
    value_type: Any

class DoubleArray(FloatingPointArray[DoubleScalar]): ...
class DoubleScalar(Scalar[float]): ...
class DurationArray(NumericArray[dt.timedelta, DurationScalar]): ...

class DurationScalar(Scalar[dt.timedelta]):
    value: Any

class DurationType(DataType[dt.timedelta]):
    unit: Literal["s", "ms", "us", "ns"]

_StorageArray = TypeVar("_StorageArray", bound=Array)

class ExtensionArray(Array, Generic[_T, _Scalar, _StorageArray]):
    storage: _StorageArray
    @staticmethod
    def from_storage(
        typ: BaseExtensionType[_T], value: Array[_T, Scalar[_T]]
    ) -> ExtensionArray[_T, _Scalar, Array[_T, Scalar[_T]]]: ...

class ExtensionScalar(Scalar[_T]):
    value: Scalar[_T]
    @staticmethod
    def from_storage(
        self, typ: BaseExtensionType[_T], value: object
    ) -> ExtensionScalar[_T]: ...

class ExtensionType(BaseExtensionType[_T]):
    def __init__(self, storage_type: DataType[_T], extension_name: str) -> None: ...
    def __arrow_ext_class__(self) -> type[ExtensionArray]: ...
    @classmethod
    def __arrow_ext_deserialize__(
        cls, storage_type: DataType[_T], serialized
    ) -> ExtensionType[_T]: ...
    def __arrow_ext_scalar_class__(self) -> type[ExtensionScalar]: ...
    def __arrow_ext_serialize__(self) -> bytes: ...
    def __eq__(self, other) -> bool: ...

_Field = TypeVar("_Field", bound=Field)

class Field(_Weakrefable, Generic[_T]):
    metadata: dict
    name: str
    nullable: bool
    type: DataType[_T]
    def _export_to_c(self, out_ptr: int) -> None: ...
    def _import_from_c(self, in_ptr: int) -> None: ...
    def equals(self, other: Field, check_metadata: bool = ...) -> bool: ...
    def flatten(self) -> list[Field]: ...
    def remove_metadata(self: _Field) -> _Field: ...
    def with_metadata(self: _Field, metadata: dict[str, str]) -> _Field: ...
    def with_name(self: _Field, name: str) -> _Field: ...
    def with_nullable(self: _Field, nullable) -> _Field: ...
    def with_type(self, new_type: DataType[_T2]) -> Field[_T2]: ...
    def __eq__(self, other) -> bool: ...

class FixedSizeBinaryArray(Array[_T, FixedSizeBinaryScalar]): ...
class FixedSizeBinaryScalar(BinaryScalar[_T]): ...

class FixedSizeBinaryType(DataType[_T]):
    byte_width: int

class FixedSizeBufferWriter(NativeFile):
    def __init__(self, buffer: Buffer) -> None: ...
    def set_memcopy_blocksize(self, blocksize: int) -> None: ...
    def set_memcopy_threads(self, num_threads: int) -> None: ...
    def set_memcopy_threshold(self, threshold: int) -> None: ...

_Values = TypeVar("_Values", bound=Array)

class FixedSizeListArray(BaseListArray, Generic[_T, _Scalar, _Values]):
    values: _Values
    @overload
    @staticmethod
    def from_arrays(
        values: _Values, type: DataType[_T] | None = ...
    ) -> FixedSizeListArray[_T, Scalar[_T], _Values]: ...
    @overload
    @staticmethod
    def from_arrays(
        values: Array[_T, Scalar[_T]], list_size: int | None = ...
    ) -> FixedSizeListArray[_T, Scalar[_T], Array[_T, Scalar[_T]]]: ...

class FixedSizeListScalar(ListScalar[_T]): ...

class FixedSizeListType(DataType[list[_T]]):
    list_size: int
    value_field: Field[_T]
    value_type: DataType[_T]

class FloatArray(FloatingPointArray[FloatScalar]): ...
class FloatScalar(Scalar[float]): ...
class FloatingPointArray(NumericArray[float, _Scalar]): ...
class HalfFloatArray(FloatingPointArray[HalfFloatScalar]): ...
class HalfFloatScalar(Scalar[float]): ...
class IOBase(_io._IOBase): ...
class Int16Array(IntegerArray[Int16Scalar]): ...
class Int16Scalar(Scalar[int]): ...
class Int32Array(IntegerArray[Int32Scalar]): ...
class Int32Scalar(Scalar[int]): ...
class Int64Array(IntegerArray[Int64Scalar]): ...
class Int64Scalar(Scalar[int]): ...
class Int8Array(IntegerArray[Int8Scalar]): ...
class Int8Scalar(Scalar[int]): ...
class IntegerArray(NumericArray[int, _Scalar]): ...

class IpcReadOptions(_Weakrefable):
    ensure_native_endian: bool
    included_fields: list | None
    use_threads: bool
    def __init__(
        self,
        *,
        use_threads: bool = ...,
        ensure_native_endian: bool = ...,
        include_fields: list | None = ...,
    ) -> None: ...

class IpcWriteOptions(_Weakrefable):
    allow_64bit: bool
    compression: str | Codec | None
    emit_dictionary_deltas: bool
    metadata_version: MetadataVersion
    unify_dictionaries: bool
    use_legacy_format: bool
    use_threads: bool
    def __init__(
        self,
        *,
        metadata_version: MetadataVersion = ...,
        allow_64bit: bool = ...,
        use_legacy_format: bool = ...,
        compression: str | Codec | None = ...,
        use_threads: bool = ...,
        emit_dictionary_details: bool = ...,
        unify_dictionaries: bool = ...,
    ) -> None: ...

class KeyValueMetadata(_Metadata, collections.abc.Mapping):
    def __init__(self, __arg0__: dict | None = ..., **kwargs) -> None: ...
    def equals(self, other) -> bool: ...
    def get_all(self, key: str) -> list: ...
    def items(self) -> ItemsView[str, Any]: ...
    def key(self, i: int) -> str: ...
    def keys(self) -> KeysView[str]: ...
    def to_dict(self) -> dict: ...
    def value(self, i: int) -> Any: ...
    def values(self) -> ValuesView[Any]: ...
    def __contains__(self, other) -> bool: ...
    def __eq__(self, other) -> bool: ...
    def __getitem__(self, key: str) -> Any: ...
    def __iter__(self) -> Generator[str, None, None]: ...
    def __len__(self) -> int: ...

class LargeBinaryArray(Array[bytes, LargeBinaryScalar]):
    total_values_length: int

class LargeBinaryScalar(BinaryScalar[bytes]): ...

class LargeListArray(BaseListArray, Generic[_T, _Scalar, _Values]):
    offsets: int
    values: _Values
    @staticmethod
    @overload
    def from_arrays(
        offsets: Int64Array,
        values: Array[_T, Scalar[_T]],
        pool: MemoryPool | None = ...,
        mask: bool | None = ...,
    ) -> LargeListArray[_T, Scalar[_T], Array[_T, Scalar[_T]]]: ...
    @staticmethod
    @overload
    def from_arrays(
        offsets: Int64Array,
        values: _Array,
        type: DataType[_T],
        pool: MemoryPool | None = ...,
        mask: bool | None = ...,
    ) -> LargeListArray[_T, Scalar[_T], _Array]: ...

class LargeListScalar(ListScalar[_T]): ...

class LargeListType(DataType[list[_T]]):
    value_field: Field[_T]
    value_type: DataType[_T]

class LargeStringArray(Array[str, LargeStringScalar]):
    @staticmethod
    def from_buffers(  # type: ignore
        length: int,
        value_offsets: Buffer,
        data: Buffer,
        null_bitmap: Buffer | None = ...,
        null_count: int = ...,
        offset: int = ...,
    ) -> LargeStringArray: ...

class LargeStringScalar(StringScalar): ...

class ListArray(BaseListArray, Generic[_T, _Scalar, _Values]):
    offsets: int
    values: _Values

    @staticmethod
    @overload
    def from_arrays(
        offsets: Int32Array,
        values: Array[_T, Scalar[_T]],
        pool: MemoryPool | None = ...,
        mask: bool | None = ...,
    ) -> ListArray[_T, Scalar[_T], Array[_T, Scalar[_T]]]: ...
    @staticmethod
    @overload
    def from_arrays(
        offsets: Int32Array,
        values: _Array,
        type: DataType[_T],
        pool: MemoryPool | None = ...,
        mask: bool | None = ...,
    ) -> ListArray[_T, Scalar[_T], _Array]: ...

class ListScalar(Scalar[list[_T]]):
    values: list[_T]

class ListType(DataType[list[_T]]):
    value_field: Field[_T]
    value_type: _T

class LoggingMemoryPool(MemoryPool): ...

_Key = TypeVar("_Key")
_Item = TypeVar("_Item")

class MapArray(
    ListArray[dict[_Key, _Item], MapScalar, StructArray], Generic[_Key, _Item]
):
    items: Array[_Item, Scalar[_Item]]
    keys: Array[_Key, Scalar[_Key]]

    @staticmethod
    def from_arrays(  # type: ignore
        offsets: Int32Array,
        keys: Array[_Key, Scalar[_Key]] | list[_Key],
        items: Array[_Item, Scalar[_Item]] | list[_Item],
        pool: MemoryPool | None = ...,
    ) -> MapArray[_Key, _Item]: ...

class MapScalar(ListScalar[dict[_Key, _Item]]): ...

class MapType(DataType[dict[_Key, _Item]]):
    item_field: Field[_Item]
    item_type: DataType[_Item]
    key_field: Field[_Key]
    key_type: DataType[_Key]

class MemoryMappedFile(NativeFile):
    def _open(self, path: str, mode: Literal["r", "r+", "w"] = ...) -> Any: ...
    @staticmethod
    def create(path: str, size: int) -> MemoryMappedFile: ...
    def fileno(self) -> int: ...
    def resize(self, new_size: int) -> None: ...

class MemoryPool(_Weakrefable):
    backend_name: str
    def bytes_allocated(self) -> int: ...
    def max_memory(self) -> int: ...
    def release_unused(self) -> None: ...

class Message(_Weakrefable):
    body: Any
    metadata: Any
    metadata_version: MetadataVersion
    type: str
    def equals(self, other: Message) -> bool: ...
    def serialize(
        self, alignment: int = ..., memory_pool: MemoryPool | None = ...
    ) -> Any: ...
    def serialize_to(
        self,
        sink: NativeFile,
        alignment: int = ...,
        memory_pool: MemoryPool | None = ...,
    ) -> None: ...

class MessageReader(_Weakrefable):
    @staticmethod
    def open_stream(source) -> MessageReader: ...
    def read_next_message(self) -> Message: ...
    def __iter__(self) -> Generator[Message, None, None]: ...

class MetadataVersion(enum.IntEnum):
    V1: ClassVar[importlib._bootstrap.MetadataVersion] = ...
    V2: ClassVar[importlib._bootstrap.MetadataVersion] = ...
    V3: ClassVar[importlib._bootstrap.MetadataVersion] = ...
    V4: ClassVar[importlib._bootstrap.MetadataVersion] = ...
    V5: ClassVar[importlib._bootstrap.MetadataVersion] = ...

class MockOutputStream(NativeFile):
    def size(self) -> int: ...

class MonthDayNano(NamedTuple):
    days: int
    months: int
    nanoseconds: int

class MonthDayNanoIntervalArray(Array[MonthDayNano, MonthDayNanoIntervalScalar]): ...

class MonthDayNanoIntervalScalar(Scalar[MonthDayNano]):
    value: MonthDayNano

_NativeFile = TypeVar("_NativeFile", bound=NativeFile)

class NativeFile(_Weakrefable):
    _default_chunk_size: ClassVar[int] = ...
    closed: bool
    mode: Literal["rb", "wb", "rb+"]
    def close(self) -> None: ...
    def download(
        self, stream_or_path: str | IOBase | NativeFile, buffer_size: int | None = ...
    ) -> None: ...
    def fileno(self) -> int: ...
    def flush(self) -> None: ...
    def get_stream(self: _NativeFile, file_offset: int, nbytes: int) -> _NativeFile: ...
    def isatty(self) -> bool: ...
    def metadata(self) -> dict: ...
    def read(self, nbytes: int | None = ...) -> bytes: ...
    def read1(self, nbytes: int | None = ...) -> bytes: ...
    def read_at(self, nbytes: int, offset: int) -> bytes: ...
    def read_buffer(self, nbytes: int | None = ...) -> Buffer: ...
    def readable(self) -> bool: ...
    def readall(self) -> bytes: ...
    def readinto(self, b: Buffer | memoryview) -> int: ...
    def readline(self, size: int = ...) -> bytes | None: ...
    def readlines(self, hint: int = ...) -> list[bytes]: ...
    def seek(self, position: int, whence: int = ...) -> None: ...
    def seekable(self) -> bool: ...
    def size(self) -> int: ...
    def tell(self) -> int: ...
    def truncate(self) -> None: ...
    def upload(self, stream: IOBase | NativeFile, buffer_size: int = ...) -> None: ...
    def writable(self) -> bool: ...
    def write(self, data: bytes | memoryview | Buffer) -> int: ...
    def writelines(self, lines: list[bytes]) -> None: ...
    def __enter__(self: _NativeFile) -> _NativeFile: ...
    def __exit__(self, exc_type, exc_value, tb) -> Any: ...
    def __iter__(self: _NativeFile) -> _NativeFile: ...
    def __next__(self) -> bytes: ...

class NullArray(Array[None, NullScalar]): ...
class NullScalar(Scalar[None]): ...
class NumericArray(Array[_T, _Scalar]): ...
class OSFile(NativeFile): ...
class ProxyMemoryPool(MemoryPool): ...

class PyExtensionType(ExtensionType[_T]):
    def __init__(self, storage_type: DataType[_T]) -> None: ...

class PythonFile(NativeFile):
    def __init__(
        self, handle: io.BytesIO, mode: Literal["rb", "wb", "rb+"] | None = ...
    ) -> None: ...

class ReadStats(importlib._bootstrap.ReadStats): ...

class RecordBatch(_PandasConvertible):
    columns: list[Array]
    nbytes: int
    num_columns: int
    num_rows: int
    schema: Schema
    def __init__(self, *args, **kwargs) -> None: ...
    def column(self, i: int) -> Array: ...
    def drop_null(self: _Self) -> _Self: ...
    def equals(self, other: RecordBatch, check_metadata: bool = ...) -> bool: ...
    def field(self, i: int) -> Field: ...
    def filter(
        self: _Self,
        mask: list[bool] | BooleanArray,
        null_selection_behavior: Literal["drop", "emit_null"] = ...,
    ) -> _Self: ...
    @staticmethod
    @overload
    def from_arrays(
        arrays: list[Array],
        *,
        names: list[str],
        metadata: dict | None = ...,
    ) -> RecordBatch: ...
    @staticmethod
    @overload
    def from_arrays(
        arrays: list[Array],
        *,
        schema: list[Schema],
        metadata: dict | None = ...,
    ) -> RecordBatch: ...
    @overload
    @staticmethod
    def from_pandas(
        df: pd.DataFrame,
        *,
        preserve_index: bool | None = ...,
        nthreads: int | None = ...,
    ) -> RecordBatch: ...
    @overload
    @staticmethod
    def from_pandas(
        df: pd.DataFrame,
        *,
        schema: Schema,
        preserve_index: bool | None = ...,
        nthreads: int | None = ...,
    ) -> RecordBatch: ...
    @overload
    @staticmethod
    def from_pandas(
        df: pd.DataFrame,
        *,
        columns: list[str],
        preserve_index: bool | None = ...,
        nthreads: int | None = ...,
    ) -> RecordBatch: ...
    @staticmethod
    def from_pydict(
        mapping: dict[str, Array | list],
        schema: Schema | None = ...,
        metadata: dict | None = ...,
    ) -> RecordBatch: ...
    @staticmethod
    def from_struct_array(struct_array: StructArray) -> RecordBatch: ...
    def get_total_buffer_size(self) -> int: ...
    def replace_schema_metadata(self: _Self, metadata: dict | None = ...) -> _Self: ...
    def serialize(self, memory_pool: MemoryPool | None = ...) -> Buffer: ...
    def slice(self: _Self, offset: int = ..., length: int | None = ...) -> _Self: ...
    def take(
        self: _Self,
        indices: list[int]
        | IntegerArray
        | NDArray[np.signedinteger | np.unsignedinteger],
    ) -> _Self: ...
    def to_pydict(self) -> dict[str, list]: ...
    def to_string(self, show_metadata: bool = ...) -> str: ...
    def validate(self, *, full: bool = ...) -> None: ...
    def __eq__(self, other) -> bool: ...
    @overload
    def __getitem__(self, key: str) -> Array: ...
    @overload
    def __getitem__(self: _Self, key: _builtin_slice) -> _Self: ...
    def __len__(self) -> int: ...
    def __sizeof__(self) -> int: ...

class RecordBatchReader(_Weakrefable):
    schema: Schema
    def __init__(self, *args, **kwargs) -> None: ...
    def _export_to_c(self, out_ptr: int) -> None: ...
    @staticmethod
    def _import_from_c(in_ptr: int) -> RecordBatchReader: ...
    def close(self) -> None: ...
    @staticmethod
    def from_batches(
        schema: Schema, batches: Iterable[RecordBatch]
    ) -> RecordBatchReader: ...
    def read_all(self) -> Table: ...
    def read_next_batch(self) -> RecordBatch: ...
    def read_pandas(self, **options) -> pd.DataFrame: ...
    def __enter__(self: _Self) -> _Self: ...
    def __exit__(self, exc_type, exc_val, exc_tb) -> None: ...
    def __iter__(self) -> Generator[RecordBatch, None, None]: ...

class ResizableBuffer(Buffer):
    def resize(self, new_size: int, shrink_to_fit: bool = ...) -> None: ...

class RuntimeInfo(NamedTuple):
    detected_simd_level: str
    simd_level: str

class Scalar(_Weakrefable, Generic[_T]):
    is_valid: bool
    type: DataType[_T]
    def __init__(self) -> None: ...
    def as_py(self) -> _T: ...
    @overload
    def cast(self, target_type: Literal["bool", "boolean"]) -> BooleanScalar: ...
    @overload
    def cast(self, target_type: Literal["i1", "int8"]) -> Int8Scalar: ...
    @overload
    def cast(self, target_type: Literal["i2", "int16"]) -> Int16Scalar: ...
    @overload
    def cast(self, target_type: Literal["i4", "int32"]) -> Int32Scalar: ...
    @overload
    def cast(self, target_type: Literal["i8", "int64"]) -> Int64Scalar: ...
    @overload
    def cast(self, target_type: Literal["u1", "uint8"]) -> UInt8Scalar: ...
    @overload
    def cast(self, target_type: Literal["u2", "uint16"]) -> UInt16Scalar: ...
    @overload
    def cast(self, target_type: Literal["u4", "uint32"]) -> UInt32Scalar: ...
    @overload
    def cast(self, target_type: Literal["u8", "uint64"]) -> UInt64Scalar: ...
    @overload
    def cast(
        self, target_type: Literal["f2", "halffloat", "float16"]
    ) -> HalfFloatScalar: ...
    @overload
    def cast(self, target_type: Literal["f4", "float", "float32"]) -> FloatScalar: ...
    @overload
    def cast(self, target_type: Literal["f8", "double", "float64"]) -> DoubleScalar: ...
    @overload
    def cast(self, target_type: Literal["string", "str", "utf8"]) -> StringScalar: ...
    @overload
    def cast(self, target_type: Literal["binary"]) -> BinaryScalar: ...
    @overload
    def cast(
        self, target_type: Literal["large_string", "large_str", "large_utf8"]
    ) -> LargeStringScalar: ...
    @overload
    def cast(self, target_type: Literal["large_binary"]) -> LargeBinaryScalar: ...
    @overload
    def cast(self, target_type: Literal["date32", "date32[day]"]) -> Date32Scalar: ...
    @overload
    def cast(self, target_type: Literal["date64", "date64[ms]"]) -> Date64Scalar: ...
    @overload
    def cast(self, target_type: Literal["time32[s]", "time32[ms]"]) -> Time32Scalar: ...
    @overload
    def cast(
        self, target_type: Literal["time64[us]", "time64[ns]"]
    ) -> Time64Scalar: ...
    @overload
    def cast(
        self,
        target_type: Literal[
            "timestamp[s]", "timestamp[ms]", "timestamp[us]", "timestamp[ns]"
        ],
    ) -> TimestampScalar: ...
    @overload
    def cast(
        self,
        target_type: Literal[
            "duration[s]", "duration[ms]", "duration[us]", "duration[ns]"
        ],
    ) -> DurationScalar: ...
    @overload
    def cast(
        self,
        target_type: Literal["month_day_nano_interval"],
    ) -> MonthDayNanoIntervalScalar: ...
    @overload
    def cast(self, target_type: DataType) -> Scalar: ...
    def equals(self, other: Scalar) -> bool: ...
    def __eq__(self, other) -> bool: ...

class Schema(_Weakrefable):
    metadata: dict[bytes, bytes] | None
    names: list[str]
    pandas_metadata: dict[str, Any] | None
    types: list[DataType]
    def _export_to_c(self, out_ptr: int) -> None: ...
    def _field(self, i: int) -> Field: ...
    @staticmethod
    def _import_from_c(in_ptr: int) -> Schema: ...
    def add_metadata(
        self: _Self, metadata: dict[str | bytes, str | bytes]
    ) -> _Self: ...
    def append(self: _Self, field: Field) -> _Self: ...
    def empty_table(self: _Self) -> _Self: ...
    def equals(self, other: Schema, check_metadata: bool = ...) -> bool: ...
    def field(self, i: int) -> Field: ...
    def field_by_name(self, name: str) -> Field | None: ...
    @classmethod
    def from_pandas(
        cls, df: pd.DataFrame, preserve_index: bool | None = ...
    ) -> Schema: ...
    def get_all_field_indices(self, name: str) -> list[int]: ...
    def get_field_index(self, name: str) -> int: ...
    def insert(self: _Self, i: int, field: Field) -> _Self: ...
    def remove(self: _Self, i: int) -> _Self: ...
    def remove_metadata(self: _Self) -> _Self: ...
    def serialize(self, memory_pool: MemoryPool | None = ...) -> Buffer: ...
    def set(self: _Self, i: int, field: Field) -> _Self: ...
    def to_string(
        self,
        truncate_metadata: bool = ...,
        show_field_metadata: bool = ...,
        show_schema_metadata: bool = ...,
    ) -> str: ...
    def with_metadata(
        self: _Self, metadata: dict[str | bytes, str | bytes]
    ) -> _Self: ...
    def __eq__(self, other) -> bool: ...
    def __getitem__(self, key: int) -> Field: ...
    def __iter__(self) -> Generator[Field, None, None]: ...
    def __len__(self) -> int: ...
    def __sizeof__(self) -> int: ...

class SerializationCallbackError(ArrowSerializationError):
    def __init__(self, message: str, example_object) -> None: ...

class SerializationContext(_Weakrefable):
    def _deserialize_callback(self, serialized_obj: dict) -> Any: ...
    def _serialize_callback(self, obj: Any) -> dict: ...
    def clone(self: _Self) -> _Self: ...
    def deserialize(self, what) -> Any: ...
    def deserialize_components(self, what) -> Any: ...
    def register_type(
        self,
        type_: type,
        type_id: str,
        pickle: bool = ...,
        custom_serializer: Callable[[Any], bytes] | None = ...,
        custom_deserializer: Callable[[bytes], Any] | None = ...,
    ) -> Any: ...
    def serialize(self, obj: Any) -> Any: ...
    def serialize_to(self, value, sink) -> Any: ...
    def set_pickle(
        self, serializer: Callable[[Any], bytes], deserializer: Callable[[bytes], Any]
    ) -> None: ...

class SerializedPyObject(_Weakrefable):
    base: Any
    total_bytes: int

    def deserialize(self, context: SerializationContext | None = ...) -> Any: ...
    @staticmethod
    def from_components(components: dict[str, Any]) -> SerializedPyObject: ...
    def to_buffer(self, nthreads: int = ...) -> Buffer: ...
    def to_components(self, memory_pool: MemoryPool | None = ...) -> dict[str, Any]: ...
    def write_to(self, sink) -> Any: ...

class SignalStopHandler:
    stop_token: StopToken
    def _init_signals(self) -> Any: ...
    def __enter__(self: _Self) -> _Self: ...
    def __exit__(self, exc_type, exc_value, exc_tb) -> None: ...

class SparseCOOTensor(_Weakrefable, Generic[_T]):
    dim_names: tuple[str, ...]
    has_canonical_format: bool
    is_mutable: bool
    ndim: int
    non_zero_length: int
    shape: tuple[int, ...]
    size: int
    type: DataType[_T]
    def dim_name(self, i: int) -> str: ...
    def equals(self, other: SparseCOOTensor) -> bool: ...
    @classmethod
    def from_dense_numpy(
        cls, obj: NDArray, dim_names: list[str] | None = ...
    ) -> SparseCOOTensor: ...
    @staticmethod
    def from_numpy(
        data: NDArray, coords: NDArray, shape: tuple, dim_names: list[str] | None = ...
    ) -> SparseCOOTensor: ...
    @staticmethod
    def from_pydata_sparse(
        obj, dim_names: list[str] | None = ...
    ) -> SparseCOOTensor: ...
    @staticmethod
    def from_scipy(obj, dim_names: list[str] | None = ...) -> SparseCOOTensor: ...
    @staticmethod
    def from_tensor(self, obj: Tensor[_T]) -> SparseCOOTensor[_T]: ...
    def to_numpy(self) -> NDArray: ...
    def to_pydata_sparse(self) -> Any: ...
    def to_scipy(self) -> Any: ...
    def to_tensor(self) -> Tensor[_T]: ...
    def __eq__(self, other) -> bool: ...

class SparseCSCMatrix(_Weakrefable, Generic[_T]):
    dim_names: tuple[str, ...]
    is_mutable: bool
    ndim: int
    non_zero_length: int
    shape: tuple[int, ...]
    size: int
    type: DataType[_T]
    def dim_name(self, i: int) -> str: ...
    def equals(self, other: SparseCSCMatrix) -> bool: ...
    @classmethod
    def from_dense_numpy(
        cls, obj: NDArray, dim_names: list[str] | None = ...
    ) -> SparseCSCMatrix: ...
    @staticmethod
    def from_numpy(
        data: NDArray,
        indptr: NDArray,
        indices: NDArray,
        shape: tuple[int, ...],
        dim_names: list[str] | None = ...,
    ) -> SparseCSCMatrix: ...
    def from_scipy(self, obj, dim_names: list[str] | None = ...) -> SparseCSCMatrix: ...
    def from_tensor(self, obj: Tensor[_T]) -> SparseCSCMatrix[_T]: ...
    def to_numpy(self) -> NDArray: ...
    def to_scipy(self) -> Any: ...
    def to_tensor(self) -> Tensor[_T]: ...
    def __eq__(self, other) -> bool: ...

class SparseCSFTensor(_Weakrefable, Generic[_T]):
    dim_names: tuple[str, ...]
    is_mutable: bool
    ndim: int
    non_zero_length: int
    shape: tuple[int, ...]
    size: int
    type: DataType[_T]
    def dim_name(self, i: int) -> str: ...
    def equals(self, other: SparseCSFTensor) -> bool: ...
    @staticmethod
    def from_dense_numpy(
        obj: NDArray, dim_names: list[str] | None = ...
    ) -> SparseCSFTensor: ...
    @staticmethod
    def from_numpy(
        data: NDArray,
        indptr: NDArray,
        indices: NDArray,
        shape: tuple[int, ...],
        axis_order: list[str] | None = ...,
        dim_names=...,
    ) -> SparseCSFTensor: ...
    @staticmethod
    def from_tensor(obj: Tensor[_T]) -> SparseCSFTensor[_T]: ...
    def to_numpy(self) -> NDArray: ...
    def to_tensor(self) -> Tensor[_T]: ...
    def __eq__(self, other) -> bool: ...

class SparseCSRMatrix(_Weakrefable, Generic[_T]):
    dim_names: tuple[str, ...]
    is_mutable: bool
    ndim: int
    non_zero_length: int
    shape: tuple[int, ...]
    size: int
    type: DataType[_T]
    def dim_name(self, i: int) -> str: ...
    def equals(self, other: SparseCSRMatrix) -> bool: ...
    @classmethod
    def from_dense_numpy(
        cls, obj: NDArray, dim_names: list[str] | None = ...
    ) -> SparseCSRMatrix: ...
    @staticmethod
    def from_numpy(
        data: NDArray,
        indptr: NDArray,
        indices: NDArray,
        shape: tuple[int, ...],
        dim_names: list[str] | None = ...,
    ) -> SparseCSRMatrix: ...
    def from_scipy(self, obj, dim_names: list[str] | None = ...) -> SparseCSRMatrix: ...
    def from_tensor(self, obj: Tensor[_T]) -> SparseCSRMatrix[_T]: ...
    def to_numpy(self) -> NDArray: ...
    def to_scipy(self) -> Any: ...
    def to_tensor(self) -> Tensor[_T]: ...
    def __eq__(self, other) -> bool: ...

class SparseUnionType(UnionType): ...
class StopToken: ...

class StringArray(Array[str, StringScalar]):
    @staticmethod
    def from_buffers(  # type: ignore
        length: int,
        value_offsets: Buffer,
        data: Buffer,
        null_bitmap: Buffer | None = ...,
        null_count: int = ...,
        offset: int = ...,
    ) -> StringArray: ...

class StringBuilder(_Weakrefable):
    null_count: int
    def __init__(self, memory_pool: MemoryPool | None = ...) -> None: ...
    def append(self, value: str | bytes) -> None: ...
    def append_values(self, values: list[str | bytes]) -> None: ...
    def finish(self) -> StringArray: ...
    def __len__(self) -> int: ...

class StringScalar(BinaryScalar[str]): ...

class StructArray(Array[dict, StructScalar]):
    def field(self, index: int | str) -> Int64Array: ...
    def flatten(self, memory_pool: MemoryPool | None = ...) -> list[Array]: ...
    @staticmethod
    def from_arrays(
        arrays: Array,
        names: list[str] | None = ...,
        fields: list[Field] | None = ...,
        mask: BooleanArray | None = ...,
        memory_pool: MemoryPool | None = ...,
    ) -> StructArray: ...

class StructScalar(Scalar, collections.abc.Mapping):
    def _as_py_tuple(self) -> Any: ...
    def as_py(self) -> dict: ...
    def items(self) -> ItemsView[str, Any]: ...
    def __contains__(self, other) -> bool: ...
    def __getitem__(self, index) -> Scalar: ...
    def __iter__(self) -> Generator[str, None, None]: ...
    def __len__(self) -> int: ...

class StructType(DataType):
    def field(self, i: int) -> Field: ...
    def get_all_field_indices(self, name: str) -> list[int]: ...
    def get_field_index(self, name: str) -> int: ...
    def __getitem__(self, index) -> Field: ...
    def __iter__(self) -> Generator[Field, None, None]: ...
    def __len__(self) -> int: ...

class Table(_PandasConvertible):
    column_names: list[str]
    columns: list[Array]
    nbytes: int
    num_columns: int
    num_rows: int
    schema: Schema
    shape: tuple[int, ...]
    def _column(self, i: int) -> Any: ...
    def _ensure_integer_index(self, i) -> Any: ...
    def _to_pandas(
        self, options, categories=..., ignore_metadata=..., types_mapper=...
    ) -> Any: ...
    def add_column(
        self: _Self, i: int, field_: str | Field, column: Array
    ) -> _Self: ...
    def append_column(self: _Self, field_: str | Field, column: Array) -> _Self: ...
    def cast(
        self,
        target_schema: Schema,
        safe: bool | None = ...,
        options: CastOptions | None = ...,
    ) -> Table: ...
    def column(self, i: int | str) -> ChunkedArray: ...
    def combine_chunks(self: _Self, memory_pool: MemoryPool | None = ...) -> _Self: ...
    def drop_null(self: _Self) -> _Self: ...
    def equals(self, other: Table, check_metadata: bool = ...) -> bool: ...
    def field(self, i: int) -> Field: ...
    def filter(
        self: _Self,
        mask: list[bool] | BooleanArray,
        null_selection_behavior: Literal["drop", "emit_null"] = ...,
    ) -> _Self: ...
    def flatten(self, memory_pool: MemoryPool | None = ...) -> Table: ...
    def from_arrays(self, arrays, names=..., schema=..., metadata=...) -> Any: ...
    def from_batches(self, batches, Schemaschema=...) -> Any: ...
    @classmethod
    def from_pandas(
        cls,
        df: pd.DataFrame,
        schema: Schema | None = ...,
        preserve_index: bool | None = ...,
        nthreads: int | None = ...,
        columns: list[str] | None = ...,
        safe: bool = ...,
    ) -> Table: ...
    @staticmethod
    def from_pydict(
        mapping: dict,
        schema: Schema | None = ...,
        metadata: dict[str | bytes, str | bytes] | None = ...,
    ) -> Table: ...
    @staticmethod
    def from_pylist(
        mapping: list[dict],
        schema: Schema | None = ...,
        metadata: dict[str | bytes, str | bytes] | None = ...,
    ) -> Table: ...
    def get_total_buffer_size(self) -> int: ...
    def group_by(self, keys: list[str]) -> TableGroupBy: ...
    def itercolumns(self) -> Generator[ChunkedArray, None, None]: ...
    def join(
        self,
        right_table: Table,
        keys: str | list[str],
        right_keys: str | list[str] | None = ...,
        join_type: Literal[
            "left semi",
            "right semi",
            "left anti",
            "right anti",
            "inner",
            "left outer",
            "right outer",
            "full outer",
        ] = ...,
        left_suffix: str | None = ...,
        right_suffix: str | None = ...,
        coalesce_keys: bool = ...,
        use_threads: bool = ...,
    ) -> Table: ...
    def remove_column(self: _Self, i: int) -> _Self: ...
    def replace_schema_metadata(
        self: _Self, metadata: dict[str | bytes, str | bytes] | None = ...
    ) -> _Self: ...
    def select(self, columns: list[str]) -> Table: ...
    def set_column(
        self: _Self, i: int, field_: str | Field, column: Array
    ) -> _Self: ...
    def slice(self: _Self, offset: int = ..., length: int | None = ...) -> _Self: ...
    def sort_by(
        self,
        sorting: Literal["ascending", "descending"]
        | list[tuple[str, Literal["ascending", "descending"]]],
    ) -> Table: ...
    def take(
        self: _Self,
        indices: list[int]
        | IntegerArray
        | NDArray[np.signedinteger | np.unsignedinteger],
    ) -> _Self: ...
    def to_batches(self, max_chunksize: int | None = ...) -> list[RecordBatch]: ...
    def to_pylist(self) -> list[dict]: ...
    def to_reader(self, max_chunksize: int | None = ...) -> RecordBatchReader: ...
    def to_string(
        self, *, show_metadata: bool = ..., preview_cols: int = ...
    ) -> str: ...
    def unify_dictionaries(
        self: _Self, memory_pool: MemoryPool | None = ...
    ) -> _Self: ...
    def validate(self, *, full: bool = ...) -> None: ...
    def __eq__(self, other) -> bool: ...
    @overload
    def __getitem__(self, index: int | str) -> ChunkedArray: ...
    @overload
    def __getitem__(self, index: _builtin_slice) -> Table: ...
    def __len__(self) -> int: ...
    def __sizeof__(self) -> int: ...

class TableGroupBy:
    def __init__(self, table: Table, keys: str | list[str]) -> None: ...
    def aggregate(
        self, aggregations: list[tuple[str, str] | tuple[str, str, FunctionOptions]]
    ) -> Table: ...

class Tensor(_Weakrefable, Generic[_T]):
    dim_names: list[str]
    is_contiguous: bool
    is_mutable: bool
    ndim: int
    shape: tuple[int, ...]
    size: int
    strides: tuple[int, ...]
    type: DataType[_T]
    def dim_name(self, i: int) -> str: ...
    def equals(self, other: Tensor) -> bool: ...
    @staticmethod
    def from_numpy(obj: NDArray, dim_names: list[str] | None = ...) -> Tensor: ...
    def to_numpy(self) -> NDArray: ...
    def __eq__(self, other) -> bool: ...

class TextIOBase(_io._TextIOBase, io.IOBase): ...
class Time32Array(NumericArray[dt.time, Time32Scalar]): ...
class Time32Scalar(Scalar[dt.time]): ...

class Time32Type(DataType[dt.time]):
    unit: str

class Time64Array(NumericArray[dt.time, Time64Scalar]): ...
class Time64Scalar(Scalar[dt.time]): ...

class Time64Type(DataType[dt.time]):
    unit: Any

class TimestampArray(NumericArray[dt.datetime, TimestampScalar]): ...

class TimestampScalar(Scalar[dt.datetime]):
    value: int

class TimestampType(DataType[dt.datetime]):
    tz: Any
    unit: str
    def to_pandas_dtype(self) -> DTypeLike: ...

class Transcoder:
    def __init__(self, decoder, encoder) -> None: ...
    def __call__(self, buf) -> Any: ...

class TransformInputStream(NativeFile): ...
class UInt16Array(IntegerArray[UInt16Scalar]): ...
class UInt16Scalar(Scalar[int]): ...
class UInt32Array(IntegerArray[UInt32Scalar]): ...
class UInt32Scalar(Scalar[int]): ...
class UInt64Array(IntegerArray[UInt64Scalar]): ...
class UInt64Scalar(Scalar[int]): ...
class UInt8Array(IntegerArray[UInt8Scalar]): ...
class UInt8Scalar(Scalar[int]): ...

class UnionArray(Array[Any, UnionScalar]):
    offsets: Int32Array
    type_codes: Int8Array
    def child(self, pos: int) -> Array: ...
    def field(self, pos: int) -> Array: ...
    @staticmethod
    def from_dense(
        types: Int8Array,
        value_offsets: Int32Array,
        children: list,
        field_names: list[str] | None = ...,
        type_codes: list | None = ...,
    ) -> UnionArray: ...
    @staticmethod
    def from_sparse(
        types: Int8Array,
        children: list,
        field_names: list[str] | None = ...,
        type_codes: list | None = ...,
    ) -> UnionArray: ...

class UnionScalar(Scalar):
    type_code: Any
    value: Any

class UnionType(DataType):
    mode: Any
    type_codes: Any
    def field(self, i) -> Field: ...
    def __getitem__(self, index) -> Any: ...
    def __iter__(self) -> Any: ...
    def __len__(self) -> int: ...

class UnknownExtensionType(PyExtensionType):
    def __arrow_ext_serialize__(self) -> Any: ...

class UnsupportedOperation(OSError, ValueError): ...

class VersionInfo(NamedTuple):
    major: str
    minor: str
    patch: str

class WriteStats(importlib._bootstrap.WriteStats):
    __slots__: ClassVar[tuple] = ...

class _CRecordBatchWriter(_Weakrefable):
    stats: Any
    def close(self) -> None: ...
    def write(self, table_or_batch: RecordBatch | Table) -> None: ...
    def write_batch(self, batch: RecordBatch) -> None: ...
    def write_table(self, table: Table, max_chunksize: int | None = ...) -> None: ...
    def __enter__(self) -> _CRecordBatchWriter: ...
    def __exit__(self, exc_type, exc_val, exc_tb) -> Any: ...

class _ExtensionRegistryNanny(_Weakrefable):
    def release_registry(self) -> None: ...

class _Metadata(_Weakrefable): ...

class _PandasAPIShim:
    _array_like_types: Any
    _categorical_type: Any
    _compat_module: Any
    _data_frame: Any
    _datetimetz_type: Any
    _extension_array: Any
    _extension_dtype: Any
    _index: Any
    _is_extension_array_dtype: Any
    _loose_version: Any
    _pd: Any
    _pd024: Any
    _series: Any
    _types_api: Any
    _version: Any
    categorical_type: Any
    compat: Any
    datetimetz_type: Any
    extension_dtype: Any
    has_sparse: Any
    have_pandas: Any
    loose_version: Any
    pd: Any
    version: Any
    def __init__(self) -> None: ...
    def assert_frame_equal(self, *args, **kwargs) -> Any: ...
    def data_frame(self, *args, **kwargs) -> pd.DataFrame: ...
    def get_rangeindex_attribute(self, level, name) -> Any: ...
    def get_values(self, obj) -> Any: ...
    def infer_dtype(self, obj) -> Any: ...
    def is_array_like(self, obj) -> bool: ...
    def is_categorical(self, obj) -> TypeGuard[pd.Categorical]: ...
    def is_data_frame(self, obj) -> TypeGuard[pd.DataFrame]: ...
    def is_datetimetz(self, obj) -> bool: ...
    def is_extension_array_dtype(self, obj) -> bool: ...
    def is_index(self, obj) -> TypeGuard[pd.Index]: ...
    def is_series(self, obj) -> TypeGuard[pd.Series]: ...
    def is_sparse(self, obj) -> bool: ...
    def pandas_dtype(self, dtype: DTypeLike) -> DTypeLike: ...
    def series(self, *args, **kwargs) -> pd.Series: ...

class _PandasConvertible(_Weakrefable):
    def to_pandas(
        self,
        memory_pool: MemoryPool | None = ...,
        categories: list[pd.Categorical] | None = ...,
        strings_to_categorical: bool | None = ...,
        zero_copy_only: bool | None = ...,
        integer_object_nulls: bool | None = ...,
        date_as_object: bool | None = ...,
        timestamp_as_object: bool | None = ...,
        use_threads: bool | None = ...,
        deduplicate_objects: bool | None = ...,
        ignore_metadata: bool | None = ...,
        safe: bool | None = ...,
        split_blocks: bool | None = ...,
        self_destruct: bool | None = ...,
        types_mapper: Callable[[DataType], pd.api.extensions.ExtensionDtype]
        | None = ...,
    ) -> pd.Series | pd.DataFrame: ...

class _ReadPandasMixin:
    def read_pandas(self, **options) -> Any: ...

class _ReadStats(NamedTuple):
    num_dictionary_batches: int
    num_dictionary_deltas: int
    num_messages: int
    num_record_batches: int
    num_replaced_dictionaries: int

class _RecordBatchFileReader(_Weakrefable):
    num_record_batches: Any
    schema: Any
    stats: Any
    @classmethod
    def __init__(self, *args, **kwargs) -> None: ...
    def _open(
        self,
        source,
        footer_offset=...,
        IpcReadOptionsoptions=...,
        MemoryPoolmemory_pool=...,
    ) -> Any: ...
    def get_batch(self, inti) -> Any: ...
    def get_record_batch(self, *args, **kwargs) -> Any: ...
    def read_all(self) -> Any: ...
    def read_pandas(self, **options) -> Any: ...
    def __enter__(self) -> Any: ...
    def __exit__(self, exc_type, exc_value, traceback) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class _RecordBatchFileWriter(_RecordBatchStreamWriter):
    @classmethod
    def __init__(self, *args, **kwargs) -> None: ...
    def _open(self, sink, Schemaschema, IpcWriteOptionsoptions=...) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class _RecordBatchStreamReader(RecordBatchReader):
    stats: Any
    def _open(
        self, source, IpcReadOptionsoptions=..., MemoryPoolmemory_pool=...
    ) -> Any: ...

class _RecordBatchStreamWriter(_CRecordBatchWriter):
    _metadata_version: Any
    _use_legacy_format: Any
    @classmethod
    def __init__(self, *args, **kwargs) -> None: ...
    def _open(self, sink, Schemaschema, IpcWriteOptionsoptions=...) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class _Weakrefable: ...

class _WriteStats(NamedTuple):
    num_dictionary_batches: int
    num_dictionary_deltas: int
    num_messages: int
    num_record_batches: int
    num_replaced_dictionaries: int

class ordered_dict:
    def __init__(self, *args, **kwargs) -> None: ...
    def clear(self, *args, **kwargs) -> Any: ...
    def copy(self) -> dict: ...
    @classmethod
    def fromkeys(cls, *args, **kwargs) -> Any: ...
    def get(self, *args, **kwargs) -> Any: ...
    def items(self, *args, **kwargs) -> Any: ...
    def keys(self, *args, **kwargs) -> Any: ...
    def pop(self, *args, **kwargs) -> Any: ...
    def popitem(self, *args, **kwargs) -> Any: ...
    def setdefault(self, *args, **kwargs) -> Any: ...
    def update(self, *args, **kwargs) -> Any: ...
    def values(self, *args, **kwargs) -> Any: ...
    @classmethod
    def __class_getitem__(cls, *args, **kwargs) -> Any: ...
    def __contains__(self, other) -> Any: ...
    def __delitem__(self, other) -> Any: ...
    def __eq__(self, other) -> Any: ...
    def __ge__(self, other) -> Any: ...
    def __getitem__(self, y) -> Any: ...
    def __gt__(self, other) -> Any: ...
    def __ior__(self, other) -> Any: ...
    def __iter__(self) -> Any: ...
    def __le__(self, other) -> Any: ...
    def __len__(self) -> Any: ...
    def __lt__(self, other) -> Any: ...
    def __ne__(self, other) -> Any: ...
    def __or__(self, other) -> Any: ...
    def __reversed__(self) -> Any: ...
    def __ror__(self, other) -> Any: ...
    def __setitem__(self, index, object) -> Any: ...
    def __sizeof__(self) -> Any: ...

def __pyx_unpickle_SerializationContext(
    __pyx_type, long__pyx_checksum, __pyx_state
) -> Any: ...
def __pyx_unpickle__PandasAPIShim(
    __pyx_type, long__pyx_checksum, __pyx_state
) -> Any: ...
def __pyx_unpickle__PandasConvertible(
    __pyx_type, long__pyx_checksum, __pyx_state
) -> Any: ...
def __pyx_unpickle___Pyx_EnumMeta(*args, **kwargs) -> Any: ...
def _datetime_from_int(int64_tvalue, TimeUnitunit, tzinfo=...) -> Any: ...
def _deprecate_serialization(name) -> Any: ...
def _deserialize(obj, SerializationContextcontext=...) -> Any: ...
def _detect_compression(path) -> Any: ...
def _empty_array(DataTypetype) -> Any: ...
def _from_pydict(cls, mapping, schema, metadata) -> Any: ...
def _from_pylist(cls, mapping, schema, metadata) -> Any: ...
def _gdb_test_session() -> Any: ...
def _get_default_context() -> Any: ...
def _handle_arrow_array_protocol(obj, type, mask, size) -> Any: ...
def _is_primitive(Typetype) -> Any: ...
def _ndarray_to_arrow_type(values, DataTypetype) -> Any: ...
def _normalize_slice(arrow_obj, slicekey) -> Any: ...
def _pc() -> Any: ...
def _read_serialized(source, base=...) -> Any: ...
def _reconstruct_record_batch(columns, schema) -> Any: ...
def _reconstruct_table(arrays, schema) -> Any: ...
def _register_py_extension_type() -> Any: ...
def _restore_array(data) -> Any: ...
def _serialize(value, SerializationContextcontext=...) -> Any: ...
def _unregister_py_extension_types() -> Any: ...
@overload
def allocate_buffer(
    size: int,
    memory_pool: MemoryPool | None = ...,
) -> Buffer: ...
@overload
def allocate_buffer(
    size: int, memory_pool: MemoryPool | None = ..., *, resizable: Literal[True]
) -> ResizableBuffer: ...
@overload
def allocate_buffer(
    size: int, memory_pool: MemoryPool | None = ..., *, resizable: Literal[False]
) -> Buffer: ...
def array(
    obj: Iterable | NDArray | pd.Series,
    type: DataType | None = ...,
    mask: list[bool] | BooleanArray | None = ...,
    size: int | None = ...,
    from_pandas: bool | None = ...,
    safe: bool = ...,
    memory_pool: MemoryPool | None = ...,
) -> Array | ChunkedArray: ...
def as_buffer(o) -> Buffer: ...
def asarray(values: Iterable, type: DataType | None = ...) -> Array: ...
def benchmark_PandasObjectIsNull(obj: list) -> Any: ...
def binary(length: int = ...) -> DataType[bytes]: ...
def bool_() -> DataType[bool]: ...
@overload
def chunked_array(arrays: Array[_T, _Scalar]) -> ChunkedArray[_T, _Scalar]: ...
@overload
def chunked_array(
    arrays: Array, type: DataType[_T]
) -> ChunkedArray[_T, Scalar[_T]]: ...
@overload
def compress(
    buf: Buffer | bytes | memoryview,
    codec: str = ...,
    *,
    memory_pool: MemoryPool | None = ...,
) -> Buffer: ...
@overload
def compress(
    buf: Buffer | bytes | memoryview,
    codec: str = ...,
    *,
    asbytes: Literal[True],
    memory_pool: MemoryPool | None = ...,
) -> bytes: ...
def concat_arrays(
    arrays: list[_Array], memory_pool: MemoryPool | None = ...
) -> _Array: ...
def concat_tables(
    tables: list[Table], promote: bool = ..., memory_pool: MemoryPool | None = ...
) -> Table: ...
def cpu_count() -> int: ...
def create_memory_map(path: str | PathLike, size: int) -> MemoryMappedFile: ...
def date32() -> DataType[dt.date]: ...
def date64() -> DataType[dt.date]: ...
def decimal128(precision: int, scale: int | None = ...) -> DataType[Decimal]: ...
def decimal256(precision: int, scale: int | None = ...) -> DataType[Decimal]: ...
@overload
def decompress(
    buf: Buffer | bytes | memoryview,
    decompressed_size: int | None = ...,
    codec: str = ...,
    *,
    memory_pool: MemoryPool | None = ...,
) -> Buffer: ...
@overload
def decompress(
    buf: Buffer | bytes | memoryview,
    decompressed_size: int | None = ...,
    codec: str = ...,
    *,
    asbytes: Literal[True],
    memory_pool: MemoryPool | None = ...,
) -> bytes: ...
def default_memory_pool() -> MemoryPool: ...
def dense_union(
    child_fields: list[Field], type_codes: list[int] | None = ...
) -> DenseUnionType: ...
def deserialize(obj, context: SerializationContext = ...) -> object: ...
def deserialize_components(
    components: dict, context: SerializationContext = ...
) -> object: ...
def deserialize_from(
    source: NativeFile, base: object, context: SerializationContext = ...
) -> object: ...
def dictionary(
    index_type: DataType, value_type: DataType, ordered: bool = ...
) -> DictionaryType: ...
def duration(unit: Literal["s", "ms", "us", "ns"]) -> DurationType: ...
def enable_signal_handlers(enable: bool) -> None: ...
def encode_file_path(path: str) -> bytes: ...
def ensure_metadata(meta: dict, allow_none: bool = ...) -> KeyValueMetadata: ...
def ensure_type(ty: DataType, allow_none=...) -> DataType: ...
def field(
    name: str | bytes,
    type: DataType[_T],
    nullable: bool = ...,
    metadata: dict | None = ...,
) -> Field[_T]: ...
def float16() -> DataType[float]: ...
def float32() -> DataType[float]: ...
def float64() -> DataType[float]: ...
def foreign_buffer(address: int, size: int, base: object | None = ...) -> Buffer: ...
def from_numpy_dtype(dtype: DTypeLike) -> DataType: ...
def frombytes(o: bytes, *, safe: bool = ...) -> str: ...
def get_record_batch_size(batch: RecordBatch) -> int: ...
def get_tensor_size(tensor: Tensor) -> int: ...
def infer_type(
    values: Iterable, mask: list[bool] | BooleanArray = ..., from_pandas: bool = ...
) -> DataType: ...
def input_stream(
    source: str | PathLike | Buffer | IOBase | NativeFile,
    compression: str | None = ...,
    buffer_size: int | None = ...,
) -> NativeFile: ...
def int16() -> DataType[int]: ...
def int32() -> DataType[int]: ...
def int64() -> DataType[int]: ...
def int8() -> DataType[int]: ...
def io_thread_count() -> int: ...
def is_boolean_value(obj: Any) -> bool: ...
def is_float_value(obj: Any) -> bool: ...
def is_integer_value(obj: Any) -> bool: ...
def is_named_tuple(cls: Any) -> bool: ...
def jemalloc_memory_pool() -> Any: ...
def jemalloc_set_decay_ms(decay_ms: int) -> None: ...
def large_binary() -> DataType[bytes]: ...
def large_list(value_type: DataType[_T] | Field[_T]) -> LargeListType[_T]: ...
def large_string() -> DataType[str]: ...
def large_utf8() -> DataType[str]: ...
def list_(
    value_type: DataType[_T] | Field[_T], list_size: int = ...
) -> ListType[_T]: ...
def log_memory_allocations(enable: bool = ...) -> None: ...
def logging_memory_pool(parent: MemoryPool) -> MemoryPool: ...
def map_(
    key_type: DataType[_Key], item_type: DataType[_Item], keys_sorted: bool = ...
) -> MapType[_Key, _Item]: ...
def memory_map(path: str, mode: Literal["r", "r+", "w"] = ...) -> MemoryMappedFile: ...
def mimalloc_memory_pool() -> MemoryPool: ...
def month_day_nano_interval() -> DataType[MonthDayNano]: ...
def null() -> DataType[None]: ...
def nulls(
    size: int, type: DataType[_T] = ..., memory_pool: MemoryPool | None = ...
) -> Array[_T, Scalar[_T]]: ...
def output_stream(
    source: str | PathLike | Buffer | IOBase | memoryview | NativeFile,
    compression: str | None = ...,
    buffer_size: int = ...,
) -> NativeFile: ...
def proxy_memory_pool(parent: MemoryPool) -> MemoryPool: ...
def py_buffer(obj: bytes | memoryview) -> Buffer: ...
def read_message(
    source: NativeFile | IOBase | memoryview | Buffer,
) -> Message: ...
def read_record_batch(
    obj: Message | Buffer | memoryview,
    schema: Schema,
    dictionary_memo: DictionaryMemo | None = ...,
) -> RecordBatch: ...
def read_schema(
    obj: Buffer | Message | memoryview, dictionary_memo: DictionaryMemo | None = ...
) -> Schema: ...
def read_serialized(source: NativeFile, base: object | None = ...) -> object: ...
def read_tensor(source: NativeFile) -> Tensor: ...
@overload
def record_batch(
    data: pd.DataFrame,
    schema: Schema | None = ...,
    metadata: dict | None = ...,
) -> RecordBatch: ...
@overload
def record_batch(
    data: list[Array | ChunkedArray],
    names: list[str],
    metadata: dict | None = ...,
) -> RecordBatch: ...
@overload
def record_batch(
    data: list[Array | ChunkedArray],
    schema: Schema,
    metadata: dict | None = ...,
) -> RecordBatch: ...
def register_extension_type(ext_type: BaseExtensionType) -> None: ...
def repeat(value, size: int, memory_pool: MemoryPool | None = ...) -> Array: ...
def runtime_info() -> RuntimeInfo: ...
def scalar(
    value: Any,
    type: DataType[_T],
    *,
    from_pandas: bool | None = ...,
    memory_pool: MemoryPool | None = ...,
) -> Scalar[_T]: ...
def schema(fields: Iterable[Field], metadata: dict | None = ...) -> Schema: ...
def serialize(value: object, context: SerializationContext | None = ...) -> object: ...
def serialize_to(
    value: object, sink: NativeFile | IOBase, context: SerializationContext | None = ...
) -> None: ...
def set_cpu_count(count: int) -> None: ...
def set_io_thread_count(count: int) -> None: ...
def set_memory_pool(pool: MemoryPool) -> None: ...
def sparse_union(
    child_fields: Iterable[Field], type_codes: list[int] = ...
) -> SparseUnionType: ...
def string() -> DataType[str]: ...
def string_to_tzinfo(name: str) -> dt.tzinfo: ...
def struct(fields: Iterable[Field]) -> StructType: ...
def supported_memory_backends() -> list[str]: ...
def system_memory_pool() -> MemoryPool: ...
@overload
def table(
    df: pd.DataFrame, schema: Schema | None = ..., nthreads: int | None = ...
) -> Table: ...
@overload
def table(
    arrays: list[Array],
    schema: Schema,
    metadata: dict | None = ...,
    nthreads: int | None = ...,
) -> Table: ...
@overload
def table(
    arrays: list[Array],
    names: list[str],
    metadata: dict | None = ...,
    nthreads: int | None = ...,
) -> Table: ...
def table_to_blocks(
    options: dict, table: Table, categories: list[str], extension_columns: list[str]
) -> list[dict]: ...
def time32(unit: Literal["s", "ms"]) -> DataType[dt.time]: ...
def time64(unit: Literal["us", "ns"]) -> DataType[dt.time]: ...
def timestamp(unit, tz=...) -> Any: ...
def tobytes(o: str | bytes) -> bytes: ...
def total_allocated_bytes() -> int: ...
def transcoding_input_stream(stream, src_encoding, dest_encoding) -> Any: ...
def type_for_alias(name: str) -> DataType: ...
def tzinfo_to_string(tz: dt.tzinfo) -> str: ...
def uint16() -> DataType[int]: ...
def uint32() -> DataType[int]: ...
def uint64() -> DataType[int]: ...
def uint8() -> DataType[int]: ...
def unify_schemas(schemas: list[Schema]) -> Schema: ...
def union(
    child_fields: Iterable[Field],
    mode: Literal["sparse", "dense"],
    type_codes: list[int] | None = ...,
) -> UnionType: ...
def unregister_extension_type(type_name: str) -> None: ...
def utf8() -> DataType[str]: ...
def write_tensor(tensor: Tensor, dest: NativeFile) -> None: ...
