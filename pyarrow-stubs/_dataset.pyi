import importlib._bootstrap  # type: ignore

from typing import Any
from typing import ClassVar
from typing import overload

import pyarrow.lib

_DEFAULT_BATCH_READAHEAD: int
_DEFAULT_BATCH_SIZE: int
_DEFAULT_FRAGMENT_READAHEAD: int
_dataset_pq: bool
_is_iterable: function
_is_path_like: function
_orc_fileformat: None
_orc_imported: bool
_stringify_path: function

class ArrowTypeError(TypeError, pyarrow.lib.ArrowException): ...

class CsvFileFormat(FileFormat):
    __slots__: ClassVar[tuple] = ...
    _read_options_py: Any
    parse_options: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def equals(self, CsvFileFormatother) -> Any: ...
    def make_write_options(self, **kwargs) -> Any: ...
    def __reduce__(self) -> Any: ...

class CsvFileWriteOptions(FileWriteOptions):
    write_options: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class CsvFragmentScanOptions(FragmentScanOptions):
    __slots__: ClassVar[tuple] = ...
    convert_options: Any
    read_options: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def equals(self, CsvFragmentScanOptionsother) -> Any: ...
    def __reduce__(self) -> Any: ...

class Dataset(pyarrow.lib._Weakrefable):
    partition_expression: Any
    schema: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def count_rows(self, **kwargs) -> Any: ...
    def get_fragments(self, Expressionfilter=...) -> Any: ...
    def head(self, intnum_rows, **kwargs) -> Any: ...
    def join(
        self,
        right_dataset,
        keys,
        right_keys=...,
        join_type=...,
        left_suffix=...,
        right_suffix=...,
        coalesce_keys=...,
        use_threads=...,
    ) -> Any: ...
    def replace_schema(self, Schemaschema) -> Any: ...
    @overload
    def scanner(self, **kwargs) -> Any: ...
    @overload
    def scanner(self, columns=...) -> Any: ...
    @overload
    def scanner(self, filter=...) -> Any: ...
    def take(self, indices, **kwargs) -> Any: ...
    def to_batches(self, **kwargs) -> Any: ...
    def to_table(self, **kwargs) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class DatasetFactory(pyarrow.lib._Weakrefable):
    root_partition: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def finish(self, Schemaschema=...) -> Any: ...
    def inspect(self) -> Any: ...
    def inspect_schemas(self) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class DirectoryPartitioning(KeyValuePartitioning):
    def __init__(self, *args, **kwargs) -> None: ...
    def discover(
        self,
        field_names=...,
        infer_dictionary=...,
        max_partition_dictionary_size=...,
        schema=...,
        segment_encoding=...,
    ) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class FeatherFileFormat(IpcFileFormat):
    default_extname: Any
    def __init__(self, *args, **kwargs) -> None: ...

class FileFormat(pyarrow.lib._Weakrefable):
    __hash__: ClassVar[None] = ...  # type: ignore
    default_extname: Any
    default_fragment_scan_options: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def inspect(self, file, filesystem=...) -> Any: ...
    def make_fragment(self, file, filesystem=..., Expressionpartition_expression=...) -> Any: ...
    def make_write_options(self) -> Any: ...
    def __eq__(self, other) -> Any: ...
    def __ge__(self, other) -> Any: ...
    def __gt__(self, other) -> Any: ...
    def __le__(self, other) -> Any: ...
    def __lt__(self, other) -> Any: ...
    def __ne__(self, other) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class FileFragment(Fragment):
    buffer: Any
    filesystem: Any
    format: Any
    path: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def open(self) -> Any: ...
    def __reduce__(self) -> Any: ...

class FileSystemDataset(Dataset):
    files: Any
    filesystem: Any
    format: Any
    partitioning: Any
    def __init__(self, *args, **kwargs) -> None: ...
    @classmethod
    def from_paths(
        cls,
        typecls,
        paths,
        schema=...,
        format=...,
        filesystem=...,
        partitions=...,
        root_partition=...,
    ) -> Any: ...
    def __reduce__(self) -> Any: ...

class FileSystemDatasetFactory(DatasetFactory):
    def __init__(self, *args, **kwargs) -> None: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class FileSystemFactoryOptions(pyarrow.lib._Weakrefable):
    __slots__: ClassVar[tuple] = ...
    exclude_invalid_files: Any
    partition_base_dir: Any
    partitioning: Any
    partitioning_factory: Any
    selector_ignore_prefixes: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class FileWriteOptions(pyarrow.lib._Weakrefable):
    format: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class FilenamePartitioning(KeyValuePartitioning):
    def __init__(self, *args, **kwargs) -> None: ...
    def discover(
        self, field_names=..., infer_dictionary=..., schema=..., segment_encoding=...
    ) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class Fragment(pyarrow.lib._Weakrefable):
    partition_expression: Any
    physical_schema: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def count_rows(self, **kwargs) -> Any: ...
    def head(self, intnum_rows, **kwargs) -> Any: ...
    def scanner(self, Schemaschema=..., **kwargs) -> Any: ...
    def take(self, indices, **kwargs) -> Any: ...
    def to_batches(self, Schemaschema=..., **kwargs) -> Any: ...
    def to_table(self, Schemaschema=..., **kwargs) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class FragmentScanOptions(pyarrow.lib._Weakrefable):
    __hash__: ClassVar[None] = ...  # type: ignore
    type_name: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def __eq__(self, other) -> Any: ...
    def __ge__(self, other) -> Any: ...
    def __gt__(self, other) -> Any: ...
    def __le__(self, other) -> Any: ...
    def __lt__(self, other) -> Any: ...
    def __ne__(self, other) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class HivePartitioning(KeyValuePartitioning):
    def __init__(self, *args, **kwargs) -> None: ...
    def discover(
        self,
        infer_dictionary=...,
        max_partition_dictionary_size=...,
        null_fallback=...,
        schema=...,
        segment_encoding=...,
    ) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class InMemoryDataset(Dataset):
    def __init__(self, *args, **kwargs) -> None: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class IpcFileFormat(FileFormat):
    default_extname: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def equals(self, IpcFileFormatother) -> Any: ...
    def __reduce__(self) -> Any: ...

class IpcFileWriteOptions(FileWriteOptions):
    def __init__(self, *args, **kwargs) -> None: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class KeyValuePartitioning(Partitioning):
    dictionaries: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class Partitioning(pyarrow.lib._Weakrefable):
    schema: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def parse(self, path) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class PartitioningFactory(pyarrow.lib._Weakrefable):
    type_name: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class RecordBatchIterator(pyarrow.lib._Weakrefable):
    def __init__(self, *args, **kwargs) -> None: ...
    def __iter__(self) -> Any: ...
    def __next__(self) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class Scanner(pyarrow.lib._Weakrefable):
    dataset_schema: Any
    projected_schema: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def count_rows(self) -> Any: ...
    def from_batches(
        self,
        source,
        Schemaschema=...,
        booluse_threads=...,
        use_async=...,
        MemoryPoolmemory_pool=...,
        columns=...,
        Expressionfilter=...,
        intbatch_size=...,
        FragmentScanOptionsfragment_scan_options=...,
    ) -> Any: ...
    def from_dataset(
        self,
        Datasetdataset,
        booluse_threads=...,
        use_async=...,
        MemoryPoolmemory_pool=...,
        columns=...,
        Expressionfilter=...,
        intbatch_size=...,
        intbatch_readahead=...,
        intfragment_readahead=...,
        FragmentScanOptionsfragment_scan_options=...,
    ) -> Any: ...
    def from_fragment(
        self,
        Fragmentfragment,
        Schemaschema=...,
        booluse_threads=...,
        use_async=...,
        MemoryPoolmemory_pool=...,
        columns=...,
        Expressionfilter=...,
        intbatch_size=...,
        intbatch_readahead=...,
        FragmentScanOptionsfragment_scan_options=...,
    ) -> Any: ...
    def head(self, intnum_rows) -> Any: ...
    def scan_batches(self) -> Any: ...
    def take(self, indices) -> Any: ...
    def to_batches(self) -> Any: ...
    def to_reader(self) -> Any: ...
    def to_table(self) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class TaggedRecordBatch(importlib._bootstrap.TaggedRecordBatch): ...

class TaggedRecordBatchIterator(pyarrow.lib._Weakrefable):
    def __init__(self, *args, **kwargs) -> None: ...
    def __iter__(self) -> Any: ...
    def __next__(self) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class UnionDataset(Dataset):
    children: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def __reduce__(self) -> Any: ...

class UnionDatasetFactory(DatasetFactory):
    def __init__(self, *args, **kwargs) -> None: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class WrittenFile(pyarrow.lib._Weakrefable):
    metadata: Any
    path: Any
    size: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

def __pyx_unpickle_WrittenFile(__pyx_type, long__pyx_checksum, __pyx_state) -> Any: ...
def _filesystemdataset_write(
    Scannerdata,
    base_dir,
    unicodebasename_template,
    FileSystemfilesystem,
    Partitioningpartitioning,
    FileWriteOptionsfile_options,
    intmax_partitions,
    file_visitor,
    unicodeexisting_data_behavior,
    intmax_open_files,
    intmax_rows_per_file,
    intmin_rows_per_group,
    intmax_rows_per_group,
    boolcreate_dir,
) -> Any: ...
def _forbid_instantiation(klass, subclasses_instead=...) -> Any: ...
def _get_orc_fileformat() -> Any: ...
def _get_parquet_classes() -> Any: ...
def _get_parquet_symbol(name) -> Any: ...
def _get_partition_keys(Expressionpartition_expression) -> Any: ...
def _pc() -> Any: ...
def frombytes(*args, **kwargs) -> Any: ...
def tobytes(o) -> Any: ...
