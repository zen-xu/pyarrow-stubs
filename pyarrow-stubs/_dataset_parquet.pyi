from typing import (
    Any,
    ClassVar,
)

import pyarrow._dataset
import pyarrow.lib

_is_path_like: function
_stringify_path: function

class ParquetDatasetFactory(pyarrow._dataset.DatasetFactory):
    def __init__(self, *args, **kwargs) -> None: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class ParquetFactoryOptions(pyarrow.lib._Weakrefable):
    __slots__: ClassVar[tuple] = ...
    partition_base_dir: Any
    partitioning: Any
    partitioning_factory: Any
    validate_column_chunk_paths: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class ParquetFileFormat(pyarrow._dataset.FileFormat):
    default_extname: Any
    read_options: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def equals(self, ParquetFileFormatother) -> Any: ...
    def make_fragment(
        self, file, filesystem=..., Expressionpartition_expression=..., row_groups=...
    ) -> Any: ...
    def make_write_options(self, **kwargs) -> Any: ...
    def __reduce__(self) -> Any: ...

class ParquetFileFragment(pyarrow._dataset.FileFragment):
    metadata: Any
    num_row_groups: Any
    row_groups: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def ensure_complete_metadata(self) -> Any: ...
    def split_by_row_group(self, Expressionfilter=..., Schemaschema=...) -> Any: ...
    def subset(
        self, Expressionfilter=..., Schemaschema=..., row_group_ids=...
    ) -> Any: ...
    def __reduce__(self) -> Any: ...

class ParquetFileWriteOptions(pyarrow._dataset.FileWriteOptions):
    def __init__(self, *args, **kwargs) -> None: ...
    def _set_arrow_properties(self) -> Any: ...
    def _set_properties(self) -> Any: ...
    def update(self, **kwargs) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class ParquetFragmentScanOptions(pyarrow._dataset.FragmentScanOptions):

    __slots__: ClassVar[tuple] = ...
    buffer_size: Any
    pre_buffer: Any
    thrift_container_size_limit: Any
    thrift_string_size_limit: Any
    use_buffered_stream: Any
    def __init__(self, *args, **kwargs) -> None: ...
    @classmethod
    def _reconstruct(cls, typecls, kwargs) -> Any: ...
    def equals(self, ParquetFragmentScanOptionsother) -> Any: ...
    def __reduce__(self) -> Any: ...

class ParquetReadOptions(pyarrow.lib._Weakrefable):
    __hash__: ClassVar[None] = ...  # type: ignore
    _coerce_int96_timestamp_unit: Any
    coerce_int96_timestamp_unit: Any
    dictionary_columns: Any
    def __init__(self, *args, **kwargs) -> None: ...
    def equals(self, ParquetReadOptionsother) -> Any: ...
    def __eq__(self, other) -> Any: ...
    def __ge__(self, other) -> Any: ...
    def __gt__(self, other) -> Any: ...
    def __le__(self, other) -> Any: ...
    def __lt__(self, other) -> Any: ...
    def __ne__(self, other) -> Any: ...
    def __reduce__(self) -> Any: ...
    def __setstate__(self, state) -> Any: ...

class RowGroupInfo:
    __hash__: ClassVar[None] = ...  # type: ignore
    def __init__(self, id, metadata, schema) -> None: ...
    def __eq__(self, other) -> Any: ...
    @property
    def num_rows(self) -> Any: ...
    @property
    def statistics(self) -> Any: ...
    @property
    def total_byte_size(self) -> Any: ...

def __pyx_unpickle_ParquetReadOptions(
    __pyx_type, long__pyx_checksum, __pyx_state
) -> Any: ...
def frombytes(*args, **kwargs) -> Any: ...
def tobytes(o) -> Any: ...
